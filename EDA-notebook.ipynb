{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cfc20fb",
   "metadata": {},
   "source": [
    "# Foundations of Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24351ed",
   "metadata": {},
   "source": [
    "## Homework outline\n",
    "This homework consists of three question, each aimed at one skill you are supposed to learn and get comfortable with during the course.\n",
    "\n",
    "In *Question 1*, you are supposed to perform an observational study based on *propensity score matching*, as you learned during the course.\n",
    "\n",
    "*Question 2*, briefly introduces you to the usage of sklearn library as well as the very basics of word vectorization.\n",
    "\n",
    "For *Question 3*, you should get familiarized with the basics of PyTorch, which is going to be of so much use during the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ed3812",
   "metadata": {},
   "source": [
    "## Question 1: Propensity score matching\n",
    "\n",
    "In this exercise, you will apply propensity score matching that was discussed the lecture (\"Observational studies\"), in order to draw conclusions from an observational study. [(a reference that could help if you are interested in the concept)](http://www.stewartschultz.com/statistics/books/Design%20of%20observational%20studies.pdf), \n",
    "\n",
    "We will work with a by-now classic dataset from Robert LaLonde's study \"[Evaluating the Econometric Evaluations of Training Programs](https://www.jstor.org/stable/1806062)\" (1986).\n",
    "The study investigated the effect of a job training program (\"National Supported Work Demonstration\") on the real earnings of an individual, a couple of years after completion of the program.\n",
    "Your task is to determine the effectiveness of the \"treatment\" represented by the job training program.\n",
    "\n",
    "#### Dataset description\n",
    "\n",
    "- `treat`: 1 if the subject participated in the job training program, 0 otherwise\n",
    "- `age`: the subject's age\n",
    "- `educ`: years of education\n",
    "- `race`: categorical variable with three possible values: Black, Hispanic, or White\n",
    "- `married`: 1 if the subject was married at the time of the training program, 0 otherwise\n",
    "- `nodegree`: 1 if the subject has earned no school degree, 0 otherwise\n",
    "- `re74`: real earnings in 1974 (pre-treatment)\n",
    "- `re75`: real earnings in 1975 (pre-treatment)\n",
    "- `re78`: real earnings in 1978 (outcome)\n",
    "\n",
    "If you want to brush up your knowledge on propensity scores and observational studies, we highly recommend Rosenbaum's excellent book on the [\"Design of Observational Studies\"](http://www.stewartschultz.com/statistics/books/Design%20of%20observational%20studies.pdf). Even just reading the first chapter (18 pages) will help you a lot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb09154",
   "metadata": {},
   "source": [
    "\n",
    "### 1-1. A naive analysis\n",
    "\n",
    "Compare the distribution of the outcome variable (`re78`) between the two groups using plots, such as histograms, and other statistical measures, such as mean, median, variance, and standard deviation.\n",
    "\n",
    "To summarize and compare the distributions, you may use the techniques we discussed in lectures 4 (\"Read the stats carefully\") and 6 (\"Data visualization\").\n",
    "\n",
    "What might a naive \"researcher\" conclude from this superficial analysis?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a14b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb43039b",
   "metadata": {},
   "source": [
    "### 1-2. A closer look at the data\n",
    "\n",
    "You're not naive, of course, so you aren't content with a superficial analysis such as the above.\n",
    "You're aware of the dangers of observational studies, so you take a closer look at the data before jumping to conclusions.\n",
    "\n",
    "For each feature in the dataset, compare its distribution in the treated group with its distribution in the control group, using plots and numbers.\n",
    "As above, you may use the techniques we discussed in class for summarizing and comparing the distributions.\n",
    "\n",
    "What do you observe?\n",
    "Describe what your observations mean for the conclusions drawn by the naive \"researcher\" from his superficial analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a81669a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5e7c38a",
   "metadata": {},
   "source": [
    "### 1-3. A propensity score model\n",
    "\n",
    "Use logistic regression to estimate propensity scores for all points in the dataset.\n",
    "You may use `sklearn` to fit the logistic regression model and apply it to each data point to obtain propensity scores:\n",
    "\n",
    "```python\n",
    "from sklearn import linear_model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "```\n",
    "\n",
    "Recall that the propensity score of a data point represents its probability of receiving the treatment, based on its pre-treatment features (in this case, age, education, pre-treatment income, etc.).\n",
    "To brush up on propensity scores, you may read chapter 3.3 of the above-cited book by Rosenbaum.\n",
    "\n",
    "Note: you do not need a train/test split here. Train and apply the model on the entire dataset. If you're wondering why this is the right thing to do in this situation, recall that the propensity score model is not used in order to make predictions about unseen data. Its sole purpose is to balance the dataset across treatment groups.\n",
    "(See p. 74 of Rosenbaum's book for an explanation why slight overfitting is even good for propensity scores.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a77ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17c8185e",
   "metadata": {},
   "source": [
    "### 1-4. Balancing the dataset via matching\n",
    "\n",
    "Use the propensity scores to match each data point from the treated group with exactly one data point from the control group, while ensuring that each data point from the control group is matched with at most one data point from the treated group.\n",
    "(Hint: you may explore the `networkx` package in Python for predefined matching functions.)\n",
    "\n",
    "Your matching should maximize the similarity between matched subjects, as captured by their propensity scores.\n",
    "In other words, the sum (over all matched pairs) of absolute propensity-score differences between the two matched subjects should be minimized.\n",
    "\n",
    "After matching, you have as many treated as you have control subjects.\n",
    "Compare the outcomes (`re78`) between the two groups (treated and control).\n",
    "\n",
    "Also, compare again the feature-value distributions between the two groups, as you've done in part 2 above, but now only for the matched subjects.\n",
    "What do you observe?\n",
    "Are you closer to being able to draw valid conclusions now than you were before?\n",
    "\n",
    "#### ** Bonus: Also, You can derive a mathematical model for this section. Your optimization problem should maximize the similarity between matched subjects, as captured by their propensity scores. In other words, the sum (over all matched pairs) of absolute propensity-score differences between the two matched subjects should be minimized.\n",
    "\n",
    "You don't need to implement this model and only need to write an optimization problem and proof theoretically that your model satisfies our goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6610b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b9045f5",
   "metadata": {},
   "source": [
    "### 1-5. Balancing the groups further\n",
    "\n",
    "Based on your comparison of feature-value distributions from part 4, are you fully satisfied with your matching?\n",
    "Would you say your dataset is sufficiently balanced?\n",
    "If not, in what ways could the \"balanced\" dataset you have obtained still not allow you to draw valid conclusions?\n",
    "\n",
    "Improve your matching by explicitly making sure that you match only subjects that have the same value for the problematic feature.\n",
    "Argue with numbers and plots that the two groups (treated and control) are now better balanced than after part 4.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01675d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae02eb93",
   "metadata": {},
   "source": [
    "### 1-6. A less naive analysis\n",
    "\n",
    "Compare the outcomes (`re78`) between treated and control subjects, as you've done in part 1, but now only for the matched dataset you've obtained from part 5.\n",
    "What do you conclude about the effectiveness of the job training program?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6769cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29650cfe",
   "metadata": {},
   "source": [
    "## Question 2: Applied ML\n",
    "\n",
    "We are going to build a classifier of news to directly assign them to 20 news categories. Note that the pipeline that you will build in this exercise could be of great help during your project!\n",
    "\n",
    "2-1. Load the 20newsgroup dataset. It is, again, a classic dataset that can directly be loaded using sklearn ([link](https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset)).  \n",
    "[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), short for term frequency–inverse document frequency (as you will see later in the course), is of great help when it comes to compute textual features. Indeed, it gives more importance to terms that are more specific to the considered articles (TF) but reduces the importance of terms that are very frequent in the entire corpus (IDF). Compute TF-IDF features for every article using [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Then, split your dataset into a training, a testing and a validation set (10% for validation and 10% for testing). Each observation should be paired with its corresponding label (the article category).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43f4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d72f6bfa",
   "metadata": {},
   "source": [
    "2-2. Train a random forest on your training set. Try to fine-tune the parameters of your predictor on your validation set using a simple grid search on the number of estimator \"n_estimators\" and the max depth of the trees \"max_depth\". Then, display a confusion matrix of your classification pipeline. Lastly, once you assessed your model, inspect the `feature_importances_` attribute of your random forest and discuss the obtained results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d1c3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59bcc4e3",
   "metadata": {},
   "source": [
    "## Question 3: CAT or PERSON; Data Sources and Introduction to Pytorch\n",
    "The purpose of this exercise is two-fold. First, you create an image dataset inheriting from `torch.utils.data.Dataset` by collecting images from two webpages providing AI-generated images to the public. This helps you to understand how data needed for a data science task might come from various sources other than locally stored images, structured files (like CSV), relational databases, etc. Also, people sometimes need to create their custom dataset for their research purposes. In the second part, you will train a neural network on the data using the pytorch framework.\n",
    "\n",
    "<img src=\"./twoGoodFriends.jpg\" width=\"400\" height=\"272\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ef38c",
   "metadata": {},
   "source": [
    "### 3-1. Installing Pytorch\n",
    "\n",
    "Your job for this section is to install pytorch and the torchvision library. You may normally need torchvision in this exercise when handling images for converting images to torch tensors. For the sake of practicing, it might be good to try installing the libraries directly from your notebook. However, you can also do it from your system shell. Whatever you do, please share your work and the installation success message below. If executing shell commands for installation please share the screenshot of your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08bf02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should insert something here for this part based on the above description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c55c88",
   "metadata": {},
   "source": [
    "### 3-2. Dataset\n",
    "Take a look at [this interesting webpage](https://thisxdoesnotexist.com/) which shares AI-generated images and have some fun first! Well, we want to collect a dataset consisting of 50 images from [this url](https://thispersondoesnotexist.com/image) created by Phillip Wang and 50 images from [this one](https://thiscatdoesnotexist.com) created by Ryan Hoover. \n",
    "For creating the dataset you must inherit from `torch.utils.data.Dataset` and name your custom dataset class as `collected_2class_Dataset`\n",
    "\n",
    "The `__init__()`, `__len__()` and `__getitem__()` methods of the class must be rewritten by you properly. \n",
    "\n",
    "The dataset must be designed in a way that by instantiation of the class, the data collection starts until all the samples are gathered. Your `__getitem__()` should also be coded in a way that `__getitem__(idx)` returns the data sample with the index `idx`. More precisely if your instance of `collected_2class_Dataset` is named `DS` and if:\n",
    "\n",
    "`img, label = DS.__getitem__(someIndex)`, Then:\n",
    "\n",
    "* `img` should be a torch tensor of size [1, 784] with data type float. The values must represent the scaled (to [0,1]) pixel values of the grayscale version of the images after being resized to 28x28.  \n",
    "* `label` should be a torch tensor of a single element with data type long taking the value of 0 if the image is for a person and 1 if the image is related to a cat.\n",
    "\n",
    "You might need to use additional packages like [Pillow](https://pypi.org/project/Pillow/) for image operations. Also, for requesting images from the websites you can follow different approaches. The simplest one might be running proper curl commands as shell commands from inside your python code. Use the following URLs for each resource to GET the images:\n",
    "* https://thispersondoesnotexist.com/image\n",
    "* https://thiscatdoesnotexist.com\n",
    "\n",
    "Write code for your Dataset class definition and also any auxiliary functions if applicable. Please instantiate your class at the end of this code section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd74d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for section 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4619aa",
   "metadata": {},
   "source": [
    "### 3-3. Dataloader\n",
    "Using `torch.utils.data.random_split` split your dataset to train/test with the proportion of 70/30. Based on `torch.utils.data.DataLoader` create a DataLoader for train data and one for test data. The choice of the batch size is up to you, but try to use rational values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31585a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for section 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f348df",
   "metadata": {},
   "source": [
    "### 3-4. Training a NN Classifier\n",
    "Inheriting from `torch.nn.Module` create a neural network for classifying the input images to person/cat. Use 2 hidden layers. Employ relu as the non-linearity (activation function) for the both hidden layers and sigmoid for the output. The choice of the number of nodes in the hidden layers is up to you. Train your network with the `torch.nn.CrossEntropyLoss` loss as objective function and use `torch.optim.Adam` as optimizer. Use 10 epochs of learning. Report the accuracy of classification at the end of each learning epoch by printing the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6b4a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for section 3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4d3597",
   "metadata": {},
   "source": [
    "### 3-5. Testing the Trained Model\n",
    "Examine the accuracy of your trained model on the test data prepared in section 3-5 and report the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae79d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for section 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db6234f",
   "metadata": {},
   "source": [
    "### 3-6. Is it Really Difficult to Distinguish Cats from Humans? \n",
    "Can you propose a rather simple rule-based system for doing this classification task? No code is needed for this part, just explain your ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e8878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for section 3.6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
